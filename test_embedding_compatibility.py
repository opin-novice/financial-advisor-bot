#!/usr/bin/env python3
"""
Test to check embedding model compatibility and cosine similarity scores
"""
import os
import sys
import numpy as np
from typing import List, Dict, Any
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def check_embedding_models():
    """Check what embedding models are being used for documents vs queries"""
    print("ğŸ” Checking Embedding Model Configuration...")
    print("=" * 80)
    
    # Check document embedding model (from docadd.py)
    try:
        with open('/Users/sayed/Downloads/final_rag/docadd.py', 'r') as f:
            docadd_content = f.read()
            
        # Extract embedding model from docadd.py
        import re
        doc_model_match = re.search(r'EMBEDDING_MODEL\s*=\s*["\']([^"\']+)["\']', docadd_content)
        doc_embedding_model = doc_model_match.group(1) if doc_model_match else "Not found"
        
        print(f"ğŸ“„ Document Embedding Model (docadd.py): {doc_embedding_model}")
        
    except Exception as e:
        print(f"âŒ Error reading docadd.py: {e}")
        doc_embedding_model = "Error reading file"
    
    # Check query embedding model (from config.py)
    try:
        from config import config
        query_embedding_model = config.EMBEDDING_MODEL
        print(f"ğŸ” Query Embedding Model (config.py): {query_embedding_model}")
        
    except Exception as e:
        print(f"âŒ Error reading config: {e}")
        query_embedding_model = "Error reading config"
    
    # Check if they match
    print("\n" + "=" * 80)
    if doc_embedding_model == query_embedding_model:
        print("âœ… MODELS MATCH: Document and query embedding models are the same")
        compatibility_status = "COMPATIBLE"
    else:
        print("âŒ MODELS MISMATCH: Document and query embedding models are different!")
        print(f"   ğŸ“„ Documents indexed with: {doc_embedding_model}")
        print(f"   ğŸ” Queries processed with: {query_embedding_model}")
        compatibility_status = "INCOMPATIBLE"
    
    return {
        'doc_model': doc_embedding_model,
        'query_model': query_embedding_model,
        'compatible': doc_embedding_model == query_embedding_model,
        'status': compatibility_status
    }

def test_cosine_similarity_scores():
    """Test and display cosine similarity scores from retrieval"""
    print("\nğŸ§ª Testing Cosine Similarity Scores in Retrieval...")
    print("=" * 80)
    
    try:
        from langchain_community.vectorstores import FAISS
        from langchain_huggingface import HuggingFaceEmbeddings
        from config import config
        
        # Initialize embeddings with current query model
        embeddings = HuggingFaceEmbeddings(
            model_name=config.EMBEDDING_MODEL, 
            model_kwargs={"device": "cpu"}
        )
        
        # Load FAISS index
        vectorstore = FAISS.load_local(
            config.FAISS_INDEX_PATH, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        
        # Test queries
        test_queries = [
            "How to open a bank account in Bangladesh?",
            "What are the loan eligibility criteria?",
            "Tax calculation methods for individuals",
            "Investment opportunities in Bangladesh"
        ]
        
        print("ğŸ“Š Cosine Similarity Scores from FAISS Retrieval:")
        print("-" * 80)
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ” Query {i}: '{query}'")
            
            # Get documents with similarity scores
            docs_with_scores = vectorstore.similarity_search_with_score(query, k=5)
            
            print(f"ğŸ“„ Retrieved {len(docs_with_scores)} documents with scores:")
            
            for j, (doc, score) in enumerate(docs_with_scores, 1):
                # FAISS returns distance, convert to similarity
                # For cosine distance: similarity = 1 - distance
                cosine_similarity = 1 - score
                
                content_preview = doc.page_content[:100].replace('\n', ' ').strip()
                if len(content_preview) > 80:
                    content_preview = content_preview[:80] + "..."
                
                print(f"   {j}. Distance: {score:.4f} | Similarity: {cosine_similarity:.4f}")
                print(f"      Content: {content_preview}")
                
                # Analyze score quality
                if cosine_similarity > 0.8:
                    quality = "ğŸŸ¢ Excellent"
                elif cosine_similarity > 0.6:
                    quality = "ğŸŸ¡ Good"
                elif cosine_similarity > 0.4:
                    quality = "ğŸŸ  Fair"
                else:
                    quality = "ğŸ”´ Poor"
                
                print(f"      Quality: {quality}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing cosine similarity scores: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_cross_encoder_scores():
    """Test cross-encoder relevance scores"""
    print("\nğŸ§ª Testing Cross-Encoder Relevance Scores...")
    print("=" * 80)
    
    try:
        from sentence_transformers import CrossEncoder
        from config import config
        
        # Initialize cross-encoder
        cross_encoder = CrossEncoder(config.CROSS_ENCODER_MODEL)
        
        # Test query-document pairs
        test_query = "How to open a bank account in Bangladesh?"
        
        # Sample documents (you can replace with actual retrieved documents)
        test_documents = [
            "To open a bank account in Bangladesh, you need to provide your NID card, passport size photos, and initial deposit amount.",
            "Investment opportunities in Bangladesh include fixed deposits, mutual funds, stock market, and real estate investments.",
            "Tax calculation for individuals in Bangladesh depends on income level, age, and residential status of the taxpayer.",
            "Loan eligibility criteria include minimum income requirements, credit history, employment status, and collateral security."
        ]
        
        print(f"ğŸ” Query: '{test_query}'")
        print("ğŸ“Š Cross-Encoder Relevance Scores:")
        print("-" * 60)
        
        # Calculate relevance scores
        pairs = [(test_query, doc) for doc in test_documents]
        scores = cross_encoder.predict(pairs)
        
        for i, (doc, score) in enumerate(zip(test_documents, scores), 1):
            doc_preview = doc[:80] + "..." if len(doc) > 80 else doc
            
            # Analyze score quality
            if score > 5.0:
                quality = "ğŸŸ¢ Highly Relevant"
            elif score > 0.0:
                quality = "ğŸŸ¡ Relevant"
            elif score > -5.0:
                quality = "ğŸŸ  Somewhat Relevant"
            else:
                quality = "ğŸ”´ Not Relevant"
            
            print(f"{i}. Score: {score:8.4f} | {quality}")
            print(f"   Doc: {doc_preview}")
            
            # Check against threshold
            threshold_status = "âœ… Above" if score >= config.RELEVANCE_THRESHOLD else "âŒ Below"
            print(f"   Threshold ({config.RELEVANCE_THRESHOLD}): {threshold_status}")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing cross-encoder scores: {e}")
        return False

def test_actual_retrieval_pipeline():
    """Test the actual retrieval pipeline to see similarity scores in action"""
    print("\nğŸ§ª Testing Actual Retrieval Pipeline...")
    print("=" * 80)
    
    try:
        from main import FinancialAdvisorTelegramBot
        
        # Initialize bot
        bot = FinancialAdvisorTelegramBot()
        
        # Test query
        test_query = "How to open a bank account in Bangladesh?"
        print(f"ğŸ” Processing query: '{test_query}'")
        
        # Process query and capture output
        import io
        import contextlib
        
        # Capture print statements
        captured_output = io.StringIO()
        with contextlib.redirect_stdout(captured_output):
            result = bot.process_query(test_query)
        
        output = captured_output.getvalue()
        
        # Extract relevance scores from output
        import re
        score_matches = re.findall(r'Document relevance score: ([\d.-]+)', output)
        
        if score_matches:
            print("ğŸ“Š Cross-Encoder Relevance Scores from Actual Pipeline:")
            for i, score in enumerate(score_matches, 1):
                score_float = float(score)
                if score_float > 5.0:
                    quality = "ğŸŸ¢ Highly Relevant"
                elif score_float > 0.0:
                    quality = "ğŸŸ¡ Relevant"
                else:
                    quality = "ğŸ”´ Not Relevant"
                
                print(f"   Document {i}: {score_float:.3f} ({quality})")
        else:
            print("âš ï¸  No relevance scores found in pipeline output")
        
        # Show result summary
        if isinstance(result, dict):
            print(f"\nâœ… Pipeline completed successfully")
            print(f"ğŸ“„ Sources found: {len(result.get('sources', []))}")
            print(f"ğŸ“ Response generated: {'Yes' if result.get('response') else 'No'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing actual retrieval pipeline: {e}")
        return False

def analyze_embedding_compatibility_impact():
    """Analyze the impact of embedding model mismatch"""
    print("\nğŸ“Š Analyzing Embedding Model Compatibility Impact...")
    print("=" * 80)
    
    compatibility_info = check_embedding_models()
    
    if not compatibility_info['compatible']:
        print("âš ï¸  CRITICAL ISSUE DETECTED: Embedding Model Mismatch")
        print("\nğŸ” Impact Analysis:")
        print("1. ğŸ“„ Documents were indexed using:", compatibility_info['doc_model'])
        print("2. ğŸ” Queries are processed using:", compatibility_info['query_model'])
        print("\nâŒ Problems this causes:")
        print("   â€¢ Cosine similarity scores will be inaccurate")
        print("   â€¢ Document retrieval quality will be poor")
        print("   â€¢ Relevant documents may not be found")
        print("   â€¢ Irrelevant documents may be ranked highly")
        
        print("\nğŸ”§ Solutions:")
        print("1. Re-index documents with the same model used for queries")
        print("2. Or change query model to match document model")
        print("3. Recommended: Use the same model for both (sentence-transformers/all-mpnet-base-v2)")
        
        return False
    else:
        print("âœ… No compatibility issues detected")
        print("ğŸ“Š Both documents and queries use the same embedding model")
        print("ğŸ¯ Cosine similarity scores should be accurate and meaningful")
        return True

def main():
    """Run all embedding compatibility tests"""
    print("ğŸš€ Embedding Model Compatibility and Similarity Score Analysis")
    print("=" * 80)
    
    # Test results tracking
    test_results = {}
    
    # Test 1: Check embedding model compatibility
    compatibility_ok = analyze_embedding_compatibility_impact()
    test_results['model_compatibility'] = compatibility_ok
    
    # Test 2: Test cosine similarity scores
    cosine_test_ok = test_cosine_similarity_scores()
    test_results['cosine_similarity_test'] = cosine_test_ok
    
    # Test 3: Test cross-encoder scores
    cross_encoder_ok = test_cross_encoder_scores()
    test_results['cross_encoder_test'] = cross_encoder_ok
    
    # Test 4: Test actual pipeline
    pipeline_ok = test_actual_retrieval_pipeline()
    test_results['pipeline_test'] = pipeline_ok
    
    # Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š EMBEDDING COMPATIBILITY TEST RESULTS:")
    print("=" * 80)
    
    for test_name, success in test_results.items():
        status = "âœ… PASS" if success else "âŒ FAIL"
        test_display = test_name.replace('_', ' ').title()
        print(f"{test_display:.<50} {status}")
    
    # Overall assessment
    total_tests = len(test_results)
    passed_tests = sum(test_results.values())
    
    print(f"\nOverall: {passed_tests}/{total_tests} tests passed")
    
    if not test_results.get('model_compatibility', True):
        print("\nğŸš¨ CRITICAL: Embedding model mismatch detected!")
        print("This will significantly impact retrieval quality.")
        print("Please fix the embedding model compatibility issue.")
        return False
    elif passed_tests == total_tests:
        print("\nğŸ‰ ALL TESTS PASSED!")
        print("Your embedding models are compatible and similarity scores are working correctly.")
        return True
    else:
        print(f"\nâš ï¸  {total_tests - passed_tests} test(s) failed.")
        print("Check the error messages above for details.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
