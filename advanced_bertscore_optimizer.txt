#!/usr/bin/env python3



"""



Cross-Encoder Optimization for RAG Pipeline



Improves document ranking and retrieval quality



"""







import os



import json



import torch



import numpy as np



from typing import List, Dict, Tuple



from sentence_transformers import CrossEncoder



from langchain.schema import Document



from sklearn.metrics.pairwise import cosine_similarity







class CrossEncoderOptimizer:



    """Optimizes document ranking using cross-encoder models"""







    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-12-v2'):



        """Initialize cross-encoder optimizer"""



        print(f"[INFO] Initializing Cross-Encoder Optimizer with model: {model_name}")



        self.model_name = model_name



        self.cross_encoder = None



        self.batch_size = 16



        self._init_cross_encoder()



        print(f"[INFO] [OK] Cross-Encoder Optimizer initialized successfully")







    def _init_cross_encoder(self):



        """Initialize the cross-encoder model"""



        try:



            print(f"[INFO] Loading cross-encoder model: {self.model_name}")



            self.cross_encoder = CrossEncoder(self.model_name, max_length=512, device='cpu')



            print(f"[INFO] [OK] Cross-encoder model loaded successfully")



        except Exception as e:



            print(f"[ERROR] Failed to load cross-encoder model: {e}")



            self.cross_encoder = None







    def rerank_documents(self, query: str, documents: List[Document], top_k: int = 5) -> List[Document]:



        """Rerank documents using cross-encoder"""



        if not documents or not self.cross_encoder:



            return documents[:top_k] if documents else []







        try:



            print(f"[INFO] [RERANK] Reranking {len(documents)} documents with cross-encoder...")



            pairs = []



            valid_docs = []



            for doc in documents:



                content = doc.page_content.strip()



                if content:



                    pairs.append([query, content])



                    valid_docs.append(doc)



            if not pairs:



                print("[WARNING] No valid document content found for reranking")



                return documents[:top_k]







            print(f"[INFO] [PREDICT] Computing relevance scores for {len(pairs)} pairs...")



            scores = self.cross_encoder.predict(pairs, batch_size=self.batch_size, convert_to_numpy=True)



            doc_scores = list(zip(valid_docs, scores))



            doc_scores.sort(key=lambda x: x[1], reverse=True)



            reranked_docs = [doc for doc, score in doc_scores[:top_k]]



            print(f"[INFO] [OK] Cross-encoder reranking completed - returning {len(reranked_docs)} documents")



            return reranked_docs



        except Exception as e:



            print(f"[ERROR] Cross-encoder reranking failed: {e}")



            return documents[:top_k]







    def optimize_document_ranking(self, query: str, documents: List[Document], threshold: float = 0.2) -> List[Document]:



        """Optimize document ranking with quality filtering"""



        if not documents:



            return []







        reranked_docs = self.rerank_documents(query, documents, top_k=len(documents))



        if not reranked_docs or not self.cross_encoder:



            return reranked_docs[:5]







        try:



            pairs = []



            valid_docs = []



            for doc in reranked_docs:



                content = doc.page_content.strip()



                if content:



                    pairs.append([query, content])



                    valid_docs.append(doc)



            if not pairs:



                return reranked_docs[:5]







            scores = self.cross_encoder.predict(pairs, batch_size=self.batch_size, convert_to_numpy=True)



            filtered_docs = []



            for doc, score in zip(valid_docs, scores):



                if score >= threshold:



                    doc.metadata['cross_encoder_score'] = float(score)



                    doc.metadata['ranking_quality'] = 'high' if score >= 0.5 else 'medium' if score >= 0.3 else 'low'



                    filtered_docs.append(doc)







            print(f"[INFO] [OPTIMIZE] Cross-encoder optimization completed - {len(filtered_docs)}/{len(reranked_docs)} documents passed threshold ({threshold})")



            return filtered_docs[:5]



        except Exception as e:



            print(f"[ERROR] Cross-encoder optimization failed: {e}")



            return reranked_docs[:5]







    def evaluate_cross_encoder_performance(self, queries: List[str], documents_sets: List[List[Document]], reference_sets: List[List[Document]]) -> Dict[str, float]:



        """Evaluate cross-encoder performance"""



        if not queries or not documents_sets or not reference_sets:



            return {"error": "Missing required evaluation data"}







        total_ndcg = 0.0



        total_map = 0.0



        total_precision = 0.0



        valid_evaluations = 0







        for query, docs, refs in zip(queries, documents_sets, reference_sets):



            try:



                reranked_docs = self.rerank_documents(query, docs, top_k=len(docs))



                ndcg = self._calculate_ndcg(reranked_docs, refs, k=5)



                map_score = self._calculate_map(reranked_docs, refs)



                precision = self._calculate_precision_at_k(reranked_docs, refs, k=5)



                total_ndcg += ndcg



                total_map += map_score



                total_precision += precision



                valid_evaluations += 1



            except Exception as e:



                print(f"[WARNING] Failed to evaluate query '{query[:50]}...': {e}")



                continue







        if valid_evaluations == 0:



            return {"error": "No valid evaluations completed"}







        avg_ndcg = total_ndcg / valid_evaluations



        avg_map = total_map / valid_evaluations



        avg_precision = total_precision / valid_evaluations







        return {



            "total_evaluations": len(queries),



            "valid_evaluations": valid_evaluations,



            "avg_ndcg@5": avg_ndcg,



            "avg_map": avg_map,



            "avg_precision@5": avg_precision,



            "performance_score": (avg_ndcg + avg_map + avg_precision) / 3



        }







    def _calculate_ndcg(self, ranked_docs: List[Document], relevant_docs: List[Document], k: int = 5) -> float:



        """Calculate NDCG@k"""



        if not ranked_docs or not relevant_docs or k <= 0:



            return 0.0



        relevance_scores = []



        relevant_sources = {doc.metadata.get('source', '') for doc in relevant_docs}



        for doc in ranked_docs[:k]:



            source = doc.metadata.get('source', '')



            relevance_scores.append(1.0 if source in relevant_sources else 0.0)



        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance_scores))



        ideal_scores = sorted(relevance_scores, reverse=True)



        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_scores))



        return dcg / idcg if idcg > 0 else 0.0







    def _calculate_map(self, ranked_docs: List[Document], relevant_docs: List[Document]) -> float:



        """Calculate Mean Average Precision"""



        if not ranked_docs or not relevant_docs:



            return 0.0



        relevant_sources = {doc.metadata.get('source', '') for doc in relevant_docs}



        num_relevant = len(relevant_sources)



        if num_relevant == 0:



            return 0.0



        ap = 0.0



        retrieved_relevant = 0



        for i, doc in enumerate(ranked_docs):



            source = doc.metadata.get('source', '')



            if source in relevant_sources:



                retrieved_relevant += 1



                precision_at_i = retrieved_relevant / (i + 1)



                ap += precision_at_i



        return ap / num_relevant







    def _calculate_precision_at_k(self, ranked_docs: List[Document], relevant_docs: List[Document], k: int = 5) -> float:



        """Calculate Precision@k"""



        if not ranked_docs or not relevant_docs or k <= 0:



            return 0.0



        relevant_sources = {doc.metadata.get('source', '') for doc in relevant_docs}



        retrieved_relevant = sum(1 for doc in ranked_docs[:k] if doc.metadata.get('source', '') in relevant_sources)



        return retrieved_relevant / k if k > 0 else 0.0







def main():



    """Main function to demonstrate cross-encoder optimization"""



    print("=" * 80)



    print("CROSS-ENCODER OPTIMIZATION DEMO")



    print("=" * 80)







    optimizer = CrossEncoderOptimizer()



    from langchain.schema import Document







    documents = [



        Document(page_content="Bangladesh Bank is the central bank that regulates monetary policy and oversees the banking system in Bangladesh.", metadata={"source": "central_bank_info.pdf", "page": 1}),



        Document(page_content="To open a savings account in Bangladesh, you need to provide your National ID and make an initial deposit at any bank branch.", metadata={"source": "account_opening_guide.pdf", "page": 2}),



        Document(page_content="Home loan interest rates in Bangladesh typically range from 8% to 12% depending on the bank and loan amount.", metadata={"source": "loan_rates_2024.pdf", "page": 3}),



        Document(page_content="Tax filing deadlines for individuals in Bangladesh are



