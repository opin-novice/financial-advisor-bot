import os
import re
import logging
import time
from typing import List, Dict, Any, Tuple
import torch
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.chains import StuffDocumentsChain, LLMChain
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from telegram import Update, File, Document as TelegramDoc, PhotoSize
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters
from sentence_transformers import CrossEncoder
import pytesseract
from PIL import Image
import fitz
import io
import warnings
warnings.filterwarnings("ignore")

# ---------- Spanish Translator Dependency -----------
from spanish_translator import SpanishTranslator

# ---------- CONFIGURATION ----------
FAISS_INDEX_PATH   = os.getenv("FAISS_INDEX_PATH", "faiss_index_multilingual")
EMBEDDING_MODEL    = os.getenv("EMBEDDING_MODEL", "sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
OLLAMA_MODEL       = os.getenv("OLLAMA_MODEL", "llama3.2:1b")
TELEGRAM_TOKEN     = os.getenv("TG_TOKEN", "YOUR_TOKEN_HERE")
CACHE_TTL          = 86400

# OCR path (edit as needed)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# Retrieval / ranking
MAX_DOCS_FOR_RETRIEVAL = 15
MAX_DOCS_FOR_CONTEXT   = 6
CONTEXT_CHUNK_SIZE     = 1800
CROSS_ENCODER_MODEL    = 'cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
RELEVANCE_THRESHOLD    = 0.15
SEMANTIC_WEIGHT        = 0.75
LEXICAL_WEIGHT         = 0.25
PHRASE_BONUS_MULTIPLIER = 1.5
LENGTH_BONUS_MULTIPLIER = 0.3

# ------------- LOGGING -------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------- CACHE -------------
class ResponseCache:
    def __init__(self, ttl=CACHE_TTL):
        self.ttl = ttl
        self.cache = {}

    def get(self, query):
        entry = self.cache.get(query)
        if entry and time.time() - entry["time"] < self.ttl:
            return entry["response"]
        return None

    def set(self, query, response):
        self.cache[query] = {"response": response, "time": time.time()}

query_cache = ResponseCache()

# ------------- EMBEDDINGS -------------
device = "cuda" if torch.cuda.is_available() else "cpu"
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={"device": device}
)

# ------------- FAISS INDEX -------------
vectorstore = FAISS.load_local(
    FAISS_INDEX_PATH,
    embeddings,
    allow_dangerous_deserialization=True
)

# ------------- RETRIEVERS -------------
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

def create_bm25_retriever():
    all_docs = vectorstore.similarity_search("", k=10000)
    bm25_retriever = BM25Retriever.from_documents(all_docs)
    bm25_retriever.k = 5
    return bm25_retriever

dense_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
bm25_retriever = create_bm25_retriever()
ensemble_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, bm25_retriever],
    weights=[SEMANTIC_WEIGHT, LEXICAL_WEIGHT]
)

# ------------- CROSS ENCODER -------------
try:
    reranker = CrossEncoder(CROSS_ENCODER_MODEL, device=device)
except Exception as e:
    logger.warning(f"Could not load CrossEncoder: {e}")
    reranker = None

# ------------- OLLAMA LLM -------------
llm = OllamaLLM(
    model=OLLAMA_MODEL,
    temperature=0.5,
    top_p=0.8,
    top_k=35,
    max_tokens=1500,
    repeat_penalty=1.2
)

# ------------- MULTILINGUAL PROMPTS -------------
BANGLA_PROMPT_TEMPLATE = """
‡¶Ü‡¶™‡¶®‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï ‡¶∏‡ßá‡¶¨‡¶æ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡ßá ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶è‡¶ï‡¶ú‡¶® ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï ‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï ‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂‡¶¶‡¶æ‡¶§‡¶æ‡•§
‡¶∏‡¶∞‡ßç‡¶¨‡¶¶‡¶æ ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶¨‡¶æ‡¶≠‡¶æ‡¶¨‡¶ø‡¶ï ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®‡•§

‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:
- ‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®
- ‡¶´‡¶∞‡ßç‡¶Æ ‡¶´‡¶ø‡¶≤‡ßç‡¶°, ‡¶ñ‡¶æ‡¶≤‡¶ø ‡¶ü‡ßá‡¶Æ‡¶™‡ßç‡¶≤‡ßá‡¶ü, ‡¶è‡¶¨‡¶Ç ‡¶Ö‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶®‡¶•‡¶ø‡¶∞ ‡¶Ö‡¶Ç‡¶∂ ‡¶â‡¶™‡ßá‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
- "‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ" ‡¶¨‡¶≤‡¶¨‡ßá‡¶® ‡¶®‡¶æ - ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®
- ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü ‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶¨‡¶≤‡ßÅ‡¶® "‡¶è ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á"
- ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßÄ ‡¶ü‡¶æ‡¶ï‡¶æ (‡ß≥/‡¶ü‡¶ï) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ø‡¶ï ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®

‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶§‡¶•‡ßç‡¶Ø:
{context}

‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {input}

‡¶â‡¶§‡ßç‡¶§‡¶∞:"""

ENGLISH_PROMPT_TEMPLATE = """
You are a helpful financial advisor specializing in Bangladesh's banking and financial services.
Always respond in a natural, conversational tone as if speaking to a friend.

IMPORTANT INSTRUCTIONS:
- Answer based on the provided information
- Ignore form fields, blank templates, placeholder text, and incomplete document fragments
- Never say "According to the context" - just answer directly
- If you don't have enough information, say "I don't have specific information about that"
- Use Bangladeshi Taka (‡ß≥/Tk) as currency
- Be concise and practical

Context Information:
{context}

Question: {input}

Answer:"""

SPANISH_PROMPT_TEMPLATE = ENGLISH_PROMPT_TEMPLATE

# ------------- LANGUAGE DETECTION & PROMPT -------------
BANGLA_KEYWORDS = ['‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶ã‡¶£', '‡¶π‡¶ø‡¶∏‡¶æ‡¶¨', '‡¶∏‡ßÅ‡¶¶', '‡¶¨‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó', '‡¶ï‡¶∞', '‡¶Ü‡¶Ø‡¶º‡¶ï‡¶∞', '‡¶∏‡¶û‡ßç‡¶ö‡¶Ø‡¶º', '‡¶Ü‡¶¨‡ßá‡¶¶‡¶®']
ENGLISH_KEYWORDS = ['bank', 'loan', 'account', 'interest', 'investment', 'tax', 'income', 'savings', 'application']
SPANISH_KEYWORDS = ['banco', 'cuenta', 'pr√©stamo', 'cr√©dito', 'dinero', 'inversi√≥n', 'impuesto', 'ahorro', 'tarjeta', 'financiero']

import langdetect
from langdetect import detect

class LanguageProcessor:
    def __init__(self):
        self.bangla_keywords = set(BANGLA_KEYWORDS)
        self.english_keywords = set(ENGLISH_KEYWORDS)
        self.spanish_keywords = set(SPANISH_KEYWORDS)
    
    def detect_language(self, text: str) -> str:
        try:
            detected = detect(text.lower())
            if detected == 'bn':
                return 'bangla'
            elif detected == 'en':
                return 'english'
            elif detected == 'es':
                return 'spanish'
        except:
            pass
        # Fallback: check for keywords or Bangla unicode
        text_lower = text.lower()
        bangla_count = sum(1 for word in self.bangla_keywords if word in text)
        english_count = sum(1 for word in self.english_keywords if word in text_lower)
        spanish_count = sum(1 for word in self.spanish_keywords if word in text_lower)
        bangla_chars = len([c for c in text if '\u0980' <= c <= '\u09FF'])
        if bangla_chars > 0 or bangla_count > max(english_count, spanish_count):
            return 'bangla'
        elif spanish_count > english_count:
            return 'spanish'
        else:
            return 'english'
    
    def get_prompt_template(self, language: str) -> PromptTemplate:
        if language == 'bangla':
            return PromptTemplate(input_variables=["context", "input"], template=BANGLA_PROMPT_TEMPLATE)
        elif language == 'spanish':
            return PromptTemplate(input_variables=["context", "input"], template=SPANISH_PROMPT_TEMPLATE)
        else:
            return PromptTemplate(input_variables=["context", "input"], template=ENGLISH_PROMPT_TEMPLATE)

lang_processor = LanguageProcessor()
spanish_translator = SpanishTranslator(OLLAMA_MODEL)

# ----------- LLM CHAINS -----------
REFORMAT_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="Rephrase the following question in one concise sentence:\n\n{question}"
)
HYDE_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="Provide a short hypothetical answer (2-3 sentences) to the question:\n\n{question}"
)
llm_chain_reformat  = LLMChain(llm=llm, prompt=REFORMAT_PROMPT)
llm_chain_hyde      = LLMChain(llm=llm, prompt=HYDE_PROMPT)

# ----------- UTILS -----------
_ESCAPE_RE = re.compile(r"([_*\[\]()~`>#+\-=|{}.!\\])")
def escape_markdown(text: str) -> str:
    return _ESCAPE_RE.sub(r"\\\1", text)

def extract_filters(query: str) -> Tuple[str, Dict[str, Any]]:
    filters: Dict[str, Any] = {}
    cleaned = query
    m = re.search(r'\bsource:([^\s]+)', query, flags=re.I)
    if m:
        filters["source"] = m.group(1).strip()
        cleaned = cleaned.replace(m.group(0), "")
    m = re.search(r'\bpage:(\d+)\b', query, flags=re.I)
    if m:
        filters["page"] = int(m.group(1))
        cleaned = cleaned.replace(m.group(0), "")
    return cleaned.strip(), filters

def expand_query(query: str) -> List[str]:
    queries = [query]
    try:
        hyde_answer = llm_chain_hyde.run(query)
        queries.append(hyde_answer)
    except: pass
    try:
        reformatted = llm_chain_reformat.run(query)
        queries.append(reformatted)
    except: pass
    return queries

def multi_query_retrieval(query: str, retriever, top_k: int = 10) -> List[Document]:
    expanded_queries = expand_query(query)
    all_docs = []
    for q in expanded_queries:
        docs = retriever.get_relevant_documents(q)
        all_docs.extend(docs)
    seen = set()
    unique_docs = []
    for doc in all_docs:
        if doc.page_content not in seen:
            seen.add(doc.page_content)
            unique_docs.append(doc)
    return unique_docs[:top_k]

def build_hypothetical_doc(question: str) -> Document:
    hypo_answer = llm_chain_hyde.run(question).strip()
    logger.info("[HyDE] Hypothetical answer: %s", hypo_answer)
    return Document(page_content=hypo_answer, metadata={"source": "Hypo_DOC"})

def rerank_documents(query: str, docs: List[Document], top_n: int = 15) -> List[Document]:
    if not reranker:
        return docs[:top_n]
    pairs = [[query, doc.page_content[:1000]] for doc in docs]
    scores = reranker.predict(pairs)
    reranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
    return [doc for score, doc in reranked[:top_n]]

def truncate_context(docs: List[Document], max_tokens: int = 4000) -> List[Document]:
    total_content = ""
    selected_docs = []
    for doc in docs:
        if len(total_content) + len(doc.page_content) < max_tokens * 4:
            total_content += doc.page_content + "\n\n"
            selected_docs.append(doc)
        else:
            break
    return selected_docs

def smart_context_selection(query: str, docs: List[Document], max_docs: int = 8) -> List[Document]:
    reranked_docs = rerank_documents(query, docs, top_n=max_docs)
    return truncate_context(reranked_docs)

def validate_response(response: str, context: str, query: str) -> dict:
    validation = {"is_grounded": False, "confidence": 0.0, "issues": []}
    if any(phrase in response.lower() for phrase in ["cannot find", "don't know", "not provided", "no information"]):
        validation["is_grounded"] = True
        validation["confidence"] = 0.95
        return validation
    response_words = set(response.lower().split())
    context_words = set(context.lower().split())
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}
    response_clean = response_words - stop_words
    context_clean = context_words - stop_words
    if len(response_clean) > 0:
        overlap = len(response_clean.intersection(context_clean))
        overlap_ratio = overlap / len(response_clean)
        length_boost = min(0.2, len(response) / 1000)
        base_confidence = min(0.9, overlap_ratio * 1.5)
        final_confidence = min(0.95, base_confidence + length_boost)
        validation["is_grounded"] = final_confidence > 0.3
        validation["confidence"] = final_confidence
        if final_confidence < 0.5:
            validation["issues"].append("Low semantic overlap with context")
    else:
        validation["confidence"] = 0.3
        validation["issues"].append("Response too short for meaningful validation")
    return validation

# ------------- MAIN QUERY PIPELINE (MULTILINGUAL) -------------
async def process_user_query(raw_query: str) -> Dict:
    # Detect language (pre-translate Spanish for all downstream chains)
    detected_language = lang_processor.detect_language(raw_query)
    orig_query = raw_query
    query_for_search = raw_query

    # Translate if Spanish
    if detected_language == 'spanish':
        logger.info("Translating Spanish query to English...")
        english_query, _ = spanish_translator.process_spanish_query(raw_query)
        query_for_search = english_query
        logger.info(f"Translated Spanish query: {english_query}")

    # Check cache
    cached = query_cache.get(orig_query)
    if cached:
        logger.info("[CACHE HIT] %s", orig_query)
        return cached

    # Filter extraction
    user_query, filters = extract_filters(query_for_search)
    logger.info(f"Parsed filters: {filters}")

    # Query reformulation (robustness)
    try:
        reformatted = llm_chain_reformat.run(user_query).strip()
    except:
        reformatted = user_query

    # Retriever (ensemble, with fallback to filtered dense if needed)
    base_retriever = ensemble_retriever
    if filters:
        base_retriever = vectorstore.as_retriever(search_kwargs={"k": 25, "filter": filters})

    # Multi-query retrieval & HyDE
    retrieved = multi_query_retrieval(reformatted, base_retriever, top_k=25)
    hypo_doc = build_hypothetical_doc(reformatted)
    retrieved.append(hypo_doc)

    # Context selection
    top_docs = smart_context_selection(reformatted, retrieved, max_docs=MAX_DOCS_FOR_CONTEXT)
    context_text = "\n".join([doc.page_content for doc in top_docs])

    # Dynamic prompt (detect again in case of OCR/PDF)
    prompt_language = detected_language
    if prompt_language == 'spanish':
        # downstream prompt must be English, since we translated
        prompt_language = 'english'
    prompt = lang_processor.get_prompt_template(prompt_language)
    llm_chain_qa = LLMChain(llm=llm, prompt=prompt)
    combine_chain = StuffDocumentsChain(
        llm_chain=llm_chain_qa,
        document_variable_name="context"
    )

    # Run QA
    try:
        answer = combine_chain.run(input_documents=top_docs, question=user_query)
    except Exception as e:
        logger.exception("LLM failed")
        answer = f"Error: {e}"

    # Translate answer back to Spanish if necessary
    if detected_language == 'spanish':
        logger.info("Translating English answer to Spanish...")
        answer = spanish_translator.process_english_response(answer, orig_query)

    # Validate
    doc_relevance_scores = []
    for doc in top_docs:
        if doc.metadata.get("source") != "Hypo_DOC":
            query_words = set(reformatted.lower().split())
            doc_words = set(doc.page_content.lower().split())
            relevance = len(query_words.intersection(doc_words)) / max(len(query_words), 1)
            doc_relevance_scores.append(relevance)
    avg_relevance = sum(doc_relevance_scores) / len(doc_relevance_scores) if doc_relevance_scores else 0.3
    validation = validate_response(answer, context_text, user_query)
    if validation["confidence"] > 0.3:
        relevance_boost = min(0.15, avg_relevance * 0.3)
        validation["confidence"] = min(0.95, validation["confidence"] + relevance_boost)

    response = {
        "result": answer,
        "source_documents": top_docs,
        "validation": validation,
        "avg_doc_relevance": avg_relevance,
        "language": detected_language
    }
    query_cache.set(orig_query, response)
    return response

# ------------- TELEGRAM HANDLERS -------------
async def start(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    welcome_msg = (
        "üáßüá© ‡¶∏‡ßç‡¶¨‡¶æ‡¶ó‡¶§‡¶Æ! ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï ‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂‡¶¶‡¶æ‡¶§‡¶æ‡•§\n"
        "üá¨üáß Welcome! I'm your financial advisor.\n"
        "üá™üá∏ ¬°Bienvenido! Soy tu asesor financiero.\n\n"
        "‡¶Ü‡¶™‡¶®‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ, ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶¨‡¶æ ‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶®‡¶ø‡¶∂ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã ‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡•§\n"
        "You can ask any financial question in Bangla, English, or Spanish.\n"
        "Puedes hacer cualquier pregunta financiera en bengal√≠, ingl√©s o espa√±ol."
    )
    await update.message.reply_text(welcome_msg)

async def handle_query(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    user_query = update.message.text.strip()
    if not user_query:
        await update.message.reply_text("Please enter a valid question. / ‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡ßà‡¶ß ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®‡•§ / Por favor ingrese una pregunta v√°lida.")
        return

    detected_lang = lang_processor.detect_language(user_query)
    processing_msg = {
        'bangla': "‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...",
        'spanish': "Procesando tu pregunta...",
        'english': "Processing your question..."
    }.get(detected_lang, "Processing your question...")

    await update.message.reply_text(processing_msg)

    response = await process_user_query(user_query)
    answer = response.get("result") if isinstance(response, dict) else str(response)
    await send_in_chunks(update, answer)

    # Send sources
    if isinstance(response, dict) and response.get("source_documents"):
        grouped = {}
        for i, doc in enumerate(response["source_documents"]):
            filename = doc.metadata.get("source", "Unknown")
            grouped.setdefault(filename, []).append(doc.page_content)

        lang = response.get("language", "english")
        header = {
            'bangla': "üìÑ ‡¶â‡ßé‡¶∏ ‡¶®‡¶•‡¶ø‡¶∏‡¶Æ‡ßÇ‡¶π:",
            'spanish': "üìÑ Documentos Fuente:",
            'english': "üìÑ Retrieved Documents:"
        }.get(lang, "üìÑ Retrieved Documents:")
        organized_output = header + "\n"
        for file, chunks in grouped.items():
            organized_output += f"\nüìÇ **{file}**\n"
            for idx, chunk in enumerate(chunks, 1):
                chunk_label = {
                    'bangla': f"‡¶Ö‡¶Ç‡¶∂ {idx}",
                    'spanish': f"Fragmento {idx}",
                    'english': f"Chunk {idx}"
                }.get(lang, f"Chunk {idx}")
                organized_output += f"\nüîπ {chunk_label}:\n{chunk}\n"
        await send_in_chunks(update, organized_output)

async def send_in_chunks(update: Update, text: str):
    MAX_LEN = 4000
    for i in range(0, len(text), MAX_LEN):
        await update.message.reply_text(text[i:i+MAX_LEN])

# ------------- PDF/IMAGE HANDLERS -------------
async def handle_pdf(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    doc: TelegramDoc = update.message.document
    if not doc.file_name.lower().endswith(".pdf"):
        await update.message.reply_text("‚ùó Please send a valid PDF document.")
        return
    file: File = await doc.get_file()
    pdf_bytes = await file.download_as_bytearray()
    try:
        pdf_doc = fitz.open("pdf", pdf_bytes)
        extracted_text = "\n".join(page.get_text() for page in pdf_doc)
        if not extracted_text.strip():
            raise ValueError("Empty PDF content")
        document = Document(page_content=extracted_text, metadata={"source": doc.file_name})
        # Language detection for OCR/PDF content!
        prompt_language = lang_processor.detect_language(extracted_text)
        prompt = lang_processor.get_prompt_template(prompt_language)
        llm_chain_qa = LLMChain(llm=llm, prompt=prompt)
        combine_chain = StuffDocumentsChain(
            llm_chain=llm_chain_qa,
            document_variable_name="context"
        )
        question = {
            "bangla": "‡¶è‡¶á PDF ‡¶ü‡¶ø ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§",
            "spanish": "Resume el PDF subido.",
            "english": "Summarize the uploaded PDF."
        }.get(prompt_language, "Summarize the uploaded PDF.")
        answer = combine_chain.run(input_documents=[document], question=question)
        await update.message.reply_text(escape_markdown(f"üìÑ *PDF Answer:*\n{answer}"), parse_mode="MarkdownV2")
    except Exception as e:
        logger.exception("PDF parsing failed")
        await update.message.reply_text(f"‚ö†Ô∏è Error processing PDF: {e}")

async def handle_image(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    photo: PhotoSize = update.message.photo[-1]
    file: File = await photo.get_file()
    img_bytes = await file.download_as_bytearray()
    try:
        image = Image.open(io.BytesIO(img_bytes))
        custom_config = r'--oem 3 --psm 6'
        extracted_text = pytesseract.image_to_string(image, config=custom_config)
        await update.message.reply_text(f"OCR text: {extracted_text}")
        if not extracted_text.strip():
            await update.message.reply_text("‚ùó Couldn't extract readable text from the image.")
            return
        document = Document(page_content=extracted_text, metadata={"source": "User_Image"})
        prompt_language = lang_processor.detect_language(extracted_text)
        prompt = lang_processor.get_prompt_template(prompt_language)
        llm_chain_qa = LLMChain(llm=llm, prompt=prompt)
        combine_chain = StuffDocumentsChain(
            llm_chain=llm_chain_qa,
            document_variable_name="context"
        )
        question = {
            "bangla": "‡¶è‡¶á ‡¶õ‡¶¨‡¶ø‡¶§‡ßá ‡¶ï‡ßÄ ‡¶Ü‡¶õ‡ßá?",
            "spanish": "¬øQu√© contiene esta imagen?",
            "english": "What does this image contain?"
        }.get(prompt_language, "What does this image contain?")
        answer = combine_chain.run(input_documents=[document], question=question)
        await update.message.reply_text(escape_markdown(f"üñºÔ∏è *Image Answer:*\n{answer}"), parse_mode="MarkdownV2")
    except Exception as e:
        logger.exception("Image OCR failed")
        await update.message.reply_text(f"‚ö†Ô∏è Error processing image: {e}")

# ------------- MAIN ENTRYPOINT -------------
if __name__ == "__main__":
    if TELEGRAM_TOKEN == "YOUR_TOKEN_HERE":
        raise RuntimeError("Please set TG_TOKEN env variable to your Telegram token.")
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_query))
    app.add_handler(MessageHandler(filters.Document.PDF, handle_pdf))
    app.add_handler(MessageHandler(filters.PHOTO, handle_image))
    logger.info("Bot started. Press Ctrl+C to stop.")
    app.run_polling()
